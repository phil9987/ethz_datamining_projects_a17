Every mapper trains an SVM on its part of the input data. For the provided training data and the map reduce framework every mapper trains on 2000 data samples. From the provided features per data point we first randomly select 128 features and then generate more features using a polynomial kernel, meaning for every selected feature we also add x[i]*x[j] for all i, j. This results in a total of 128^2=16384 features. The weights of every mapper are initialized according to a standard normal distribution. We use stochastic gradient descent (SGD) where the weights are updated with the ADAM (a combination of AdaGrad and Momentum) strategy. As a loss function we use hinge loss. The reducer receives the trained weights of every mapper and returns the average of it, which then is the end result. 
The above mentioned parameter and architecture choices were found using grid search and with various experiments. We also explored the RBF kernel but could not achieve results above the hard baseline.